{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540c2111",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-19T08:28:02.680464Z",
     "iopub.status.busy": "2025-11-19T08:28:02.680185Z",
     "iopub.status.idle": "2025-11-19T08:28:25.868904Z",
     "shell.execute_reply": "2025-11-19T08:28:25.868145Z"
    },
    "papermill": {
     "duration": 23.195482,
     "end_time": "2025-11-19T08:28:25.870446",
     "exception": false,
     "start_time": "2025-11-19T08:28:02.674964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torchvision import transforms, datasets\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dab28b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T08:28:25.879083Z",
     "iopub.status.busy": "2025-11-19T08:28:25.878814Z",
     "iopub.status.idle": "2025-11-19T08:28:25.976887Z",
     "shell.execute_reply": "2025-11-19T08:28:25.976238Z"
    },
    "papermill": {
     "duration": 0.103608,
     "end_time": "2025-11-19T08:28:25.978124",
     "exception": false,
     "start_time": "2025-11-19T08:28:25.874516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"/kaggle/input/malimg-original/malimg_paper_dataset_imgs\"\n",
    "BATCH_SIZE = 16  \n",
    "IMG_SIZE = 256   # Sesuai model SwinV2-256\n",
    "NUM_CLASSES = 25\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "OUTPUT_DIR = \"/kaggle/working/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255df77a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T08:28:25.986259Z",
     "iopub.status.busy": "2025-11-19T08:28:25.985944Z",
     "iopub.status.idle": "2025-11-19T08:29:17.160392Z",
     "shell.execute_reply": "2025-11-19T08:29:17.159299Z"
    },
    "papermill": {
     "duration": 51.180418,
     "end_time": "2025-11-19T08:29:17.162085",
     "exception": false,
     "start_time": "2025-11-19T08:28:25.981667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "temp_dataset = datasets.ImageFolder(root=DATA_DIR, transform=temp_transform)\n",
    "loader = DataLoader(temp_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_samples = 0\n",
    "for data, _ in loader:\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    total_samples += batch_samples\n",
    "mean /= total_samples\n",
    "std /= total_samples\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((int(IMG_SIZE * 1.1), int(IMG_SIZE * 1.1))),\n",
    "    transforms.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.95, 1.0), ratio=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.3),\n",
    "    transforms.RandomApply([\n",
    "        transforms.Lambda(lambda x: torch.clamp(x + torch.randn_like(x) * 0.01, 0, 1))\n",
    "    ], p=0.3),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f23d617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T08:29:17.170895Z",
     "iopub.status.busy": "2025-11-19T08:29:17.170616Z",
     "iopub.status.idle": "2025-11-19T08:29:19.917349Z",
     "shell.execute_reply": "2025-11-19T08:29:19.916731Z"
    },
    "papermill": {
     "duration": 2.752914,
     "end_time": "2025-11-19T08:29:19.918836",
     "exception": false,
     "start_time": "2025-11-19T08:29:17.165922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dataset = datasets.ImageFolder(root=DATA_DIR)\n",
    "all_targets = [label for _, label in raw_dataset.samples]\n",
    "all_indices = list(range(len(raw_dataset)))\n",
    "\n",
    "split_path = os.path.join(OUTPUT_DIR, \"train_test_split.npz\")\n",
    "if os.path.exists(split_path):\n",
    "    splits = np.load(split_path)\n",
    "    train_idx, test_idx = splits['train_idx'], splits['test_idx']\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        all_indices,\n",
    "        test_size=0.2,\n",
    "        stratify=all_targets,\n",
    "        random_state=42\n",
    "    )\n",
    "    np.savez(split_path, train_idx=train_idx, test_idx=test_idx)\n",
    "\n",
    "train_dataset_raw = datasets.ImageFolder(root=DATA_DIR, transform=train_transform)\n",
    "test_dataset_raw = datasets.ImageFolder(root=DATA_DIR, transform=test_transform)\n",
    "\n",
    "train_dataset = Subset(train_dataset_raw, train_idx)\n",
    "test_dataset = Subset(test_dataset_raw, test_idx)\n",
    "\n",
    "train_targets = [all_targets[i] for i in train_idx]\n",
    "class_counts = Counter(train_targets)\n",
    "weights = [1.0 / class_counts[train_targets[i]] for i in range(len(train_targets))]\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "class_names = raw_dataset.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2229e5e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T08:29:19.927545Z",
     "iopub.status.busy": "2025-11-19T08:29:19.927235Z",
     "iopub.status.idle": "2025-11-19T08:29:24.739961Z",
     "shell.execute_reply": "2025-11-19T08:29:24.739233Z"
    },
    "papermill": {
     "duration": 4.818905,
     "end_time": "2025-11-19T08:29:24.741686",
     "exception": false,
     "start_time": "2025-11-19T08:29:19.922781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39bf715c99e4556b1b5cfe85f955bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/357M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformerV2(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(1, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerV2Stage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=4, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=4, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.004)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.004)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.013)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.013)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.017)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.017)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.022)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "        (2): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.026)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.026)\n",
       "        )\n",
       "        (3): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.030)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.030)\n",
       "        )\n",
       "        (4): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.035)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.035)\n",
       "        )\n",
       "        (5): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.039)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.039)\n",
       "        )\n",
       "        (6): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.043)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.043)\n",
       "        )\n",
       "        (7): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.048)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.048)\n",
       "        )\n",
       "        (8): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.052)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.052)\n",
       "        )\n",
       "        (9): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.057)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.057)\n",
       "        )\n",
       "        (10): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (11): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.065)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.065)\n",
       "        )\n",
       "        (12): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.070)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.070)\n",
       "        )\n",
       "        (13): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.074)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.074)\n",
       "        )\n",
       "        (14): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (15): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.083)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.083)\n",
       "        )\n",
       "        (16): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.087)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.087)\n",
       "        )\n",
       "        (17): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=32, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=32, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=25, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('swinv2_base_window16_256', pretrained=True, num_classes=NUM_CLASSES)\n",
    "\n",
    "model.patch_embed.proj = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=model.patch_embed.proj.out_channels,\n",
    "    kernel_size=model.patch_embed.proj.kernel_size,\n",
    "    stride=model.patch_embed.proj.stride,\n",
    "    bias=model.patch_embed.proj.bias is not None\n",
    ")\n",
    "nn.init.xavier_uniform_(model.patch_embed.proj.weight)\n",
    "if model.patch_embed.proj.bias is not None:\n",
    "    nn.init.zeros_(model.patch_embed.proj.bias)\n",
    "\n",
    "#dropout dihead\n",
    "model.head = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(model.head.in_features, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce26f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T08:29:24.751439Z",
     "iopub.status.busy": "2025-11-19T08:29:24.751183Z",
     "iopub.status.idle": "2025-11-19T08:29:24.765964Z",
     "shell.execute_reply": "2025-11-19T08:29:24.765224Z"
    },
    "papermill": {
     "duration": 0.020954,
     "end_time": "2025-11-19T08:29:24.767173",
     "exception": false,
     "start_time": "2025-11-19T08:29:24.746219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Pastikan inputs adalah 2D [batch_size, num_classes]\n",
    "        if inputs.dim() > 2:\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "        \n",
    "        # Pastikan targets adalah 1D tensor dengan tipe long\n",
    "        targets = targets.view(-1).long()\n",
    "        \n",
    "        # Validasi: pastikan targets dalam range yang valid [0, num_classes-1]\n",
    "        num_classes = inputs.size(1)\n",
    "        assert targets.min() >= 0, f\"Target min value {targets.min()} is negative\"\n",
    "        assert targets.max() < num_classes, f\"Target max value {targets.max()} >= num_classes {num_classes}\"\n",
    "        \n",
    "        # Hitung cross entropy dengan log softmax\n",
    "        log_probs = torch.nn.functional.log_softmax(inputs, dim=-1)\n",
    "        \n",
    "        # Gunakan cross_entropy internal untuk lebih aman\n",
    "        ce_loss = torch.nn.functional.nll_loss(log_probs, targets, reduction='none')\n",
    "        \n",
    "        # Hitung probability untuk focal weight\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Hitung focal loss\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "criterion = FocalLoss(gamma=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe22c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T08:29:24.776641Z",
     "iopub.status.busy": "2025-11-19T08:29:24.776361Z",
     "iopub.status.idle": "2025-11-19T08:29:27.510625Z",
     "shell.execute_reply": "2025-11-19T08:29:27.509387Z"
    },
    "papermill": {
     "duration": 2.740723,
     "end_time": "2025-11-19T08:29:27.512237",
     "exception": false,
     "start_time": "2025-11-19T08:29:24.771514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDASI DATA\n",
      "============================================================\n",
      "Number of classes in dataset: 25\n",
      "Class names: ['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Allaple.L', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.P', 'C2LOP.gen!g', 'Dialplatform.B', 'Dontovo.A', 'Fakerean', 'Instantaccess', 'Lolyda.AA1', 'Lolyda.AA2', 'Lolyda.AA3', 'Lolyda.AT', 'Malex.gen!J', 'Obfuscator.AD', 'Rbot!gen', 'Skintrim.N', 'Swizzor.gen!E', 'Swizzor.gen!I', 'VB.AT', 'Wintrim.BX', 'Yuner.A']\n",
      "NUM_CLASSES parameter: 25\n",
      "Train samples: 7471\n",
      "Test samples: 1868\n",
      "\n",
      "Checking training labels...\n",
      "Sample labels min: 0, max: 24\n",
      "Unique labels in sample: [ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 20 21 22 23 24]\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Testing forward pass...\n",
      "Input shape: torch.Size([16, 1, 256, 256])\n",
      "Output shape: torch.Size([16, 8, 8, 25])\n",
      "Label shape: torch.Size([16])\n",
      "Expected output shape: [16, 25]\n",
      "  Output has 4 dimensions, reshaping...\n",
      "Reshaped output: torch.Size([16, 1600])\n",
      "\n",
      "Forward pass test successful! Starting training...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VALIDASI DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of classes in dataset: {len(class_names)}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"NUM_CLASSES parameter: {NUM_CLASSES}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Cek range label di training set\n",
    "print(\"\\nChecking training labels...\")\n",
    "sample_labels = []\n",
    "for i, (_, label) in enumerate(train_dataset):\n",
    "    sample_labels.append(label)\n",
    "    if i >= 100:  # Check first 100 samples\n",
    "        break\n",
    "\n",
    "sample_labels = np.array(sample_labels)\n",
    "print(f\"Sample labels min: {sample_labels.min()}, max: {sample_labels.max()}\")\n",
    "print(f\"Unique labels in sample: {np.unique(sample_labels)}\")\n",
    "\n",
    "# Verifikasi apakah NUM_CLASSES sesuai dengan jumlah kelas aktual\n",
    "actual_num_classes = len(class_names)\n",
    "if NUM_CLASSES != actual_num_classes:\n",
    "    print(f\"\\n  WARNING: NUM_CLASSES ({NUM_CLASSES}) != actual classes ({actual_num_classes})\")\n",
    "    print(f\"Updating NUM_CLASSES to {actual_num_classes}\")\n",
    "    NUM_CLASSES = actual_num_classes\n",
    "    \n",
    "    # Rebuild model head\n",
    "    model.head = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(model.head[1].in_features, NUM_CLASSES)\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Rebuild optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test forward pass untuk debug\n",
    "print(\"\\nTesting forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(train_loader))\n",
    "    test_input, test_label = test_batch[0].to(DEVICE), test_batch[1].to(DEVICE)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    print(f\"Output shape: {test_output.shape}\")\n",
    "    print(f\"Label shape: {test_label.shape}\")\n",
    "    print(f\"Expected output shape: [{test_input.size(0)}, {NUM_CLASSES}]\")\n",
    "    \n",
    "    if test_output.dim() > 2:\n",
    "        print(f\"  Output has {test_output.dim()} dimensions, reshaping...\")\n",
    "        test_output = test_output.view(test_output.size(0), -1)\n",
    "        print(f\"Reshaped output: {test_output.shape}\")\n",
    "\n",
    "print(\"\\nForward pass test successful! Starting training...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6f9e05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T08:29:27.524578Z",
     "iopub.status.busy": "2025-11-19T08:29:27.523875Z",
     "iopub.status.idle": "2025-11-19T11:37:00.207275Z",
     "shell.execute_reply": "2025-11-19T11:37:00.206274Z"
    },
    "papermill": {
     "duration": 11252.696666,
     "end_time": "2025-11-19T11:37:00.214298",
     "exception": false,
     "start_time": "2025-11-19T08:29:27.517632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] First batch - Output shape: torch.Size([16, 8, 8, 25])\n",
      "[DEBUG] Reshaped to: torch.Size([16, 1600])\n",
      "\n",
      "Epoch 1/20 - Train Loss: 0.7482, Train Acc: 75.28%, Val Loss: 0.0394, Val Acc: 90.42%\n",
      "Epoch 2/20 - Train Loss: 0.0676, Train Acc: 92.56%, Val Loss: 0.0277, Val Acc: 90.58%\n",
      "Epoch 3/20 - Train Loss: 0.0457, Train Acc: 94.08%, Val Loss: 0.8168, Val Acc: 81.48%\n",
      "Epoch 4/20 - Train Loss: 0.0465, Train Acc: 94.23%, Val Loss: 0.0181, Val Acc: 99.36%\n",
      "Epoch 5/20 - Train Loss: 0.0454, Train Acc: 93.98%, Val Loss: 0.0280, Val Acc: 91.06%\n",
      "Epoch 6/20 - Train Loss: 0.0318, Train Acc: 95.33%, Val Loss: 0.0244, Val Acc: 98.98%\n",
      "Epoch 7/20 - Train Loss: 0.0279, Train Acc: 96.57%, Val Loss: 0.0180, Val Acc: 99.14%\n",
      "Epoch 8/20 - Train Loss: 0.0303, Train Acc: 96.05%, Val Loss: 0.0104, Val Acc: 99.25%\n",
      "Epoch 9/20 - Train Loss: 0.0307, Train Acc: 96.56%, Val Loss: 0.0200, Val Acc: 99.52%\n",
      "Epoch 10/20 - Train Loss: 0.0186, Train Acc: 97.60%, Val Loss: 0.0080, Val Acc: 99.52%\n",
      "Epoch 11/20 - Train Loss: 0.0209, Train Acc: 97.28%, Val Loss: 0.0094, Val Acc: 99.46%\n",
      "Epoch 12/20 - Train Loss: 0.0129, Train Acc: 98.35%, Val Loss: 0.0088, Val Acc: 98.93%\n",
      "Epoch 13/20 - Train Loss: 0.0139, Train Acc: 98.17%, Val Loss: 0.0133, Val Acc: 99.25%\n",
      "Epoch 14/20 - Train Loss: 0.0275, Train Acc: 96.97%, Val Loss: 0.0090, Val Acc: 99.30%\n",
      "Epoch 15/20 - Train Loss: 0.0069, Train Acc: 98.77%, Val Loss: 0.0076, Val Acc: 99.41%\n",
      "Epoch 16/20 - Train Loss: 0.0073, Train Acc: 98.69%, Val Loss: 0.0050, Val Acc: 99.57%\n",
      "Epoch 17/20 - Train Loss: 0.0072, Train Acc: 98.77%, Val Loss: 0.0074, Val Acc: 99.57%\n",
      "Epoch 18/20 - Train Loss: 0.0052, Train Acc: 99.10%, Val Loss: 0.0067, Val Acc: 99.46%\n",
      "Epoch 19/20 - Train Loss: 0.0075, Train Acc: 98.97%, Val Loss: 0.0071, Val Acc: 99.52%\n",
      "Epoch 20/20 - Train Loss: 0.0086, Train Acc: 98.86%, Val Loss: 0.0050, Val Acc: 99.46%\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # DEBUG: Print shape di first batch\n",
    "        if epoch == 0 and total_train == 0:\n",
    "            print(f\"\\n[DEBUG] First batch - Output shape: {outputs.shape}\")\n",
    "        \n",
    "        # CRITICAL FIX: Reshape jika output bukan 2D\n",
    "        if outputs.dim() > 2:\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            if epoch == 0 and total_train == 0:\n",
    "                print(f\"[DEBUG] Reshaped to: {outputs.shape}\\n\")\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # CRITICAL FIX: Reshape jika output bukan 2D\n",
    "            if outputs.dim() > 2:\n",
    "                outputs = outputs.view(outputs.size(0), -1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * correct_val / total_val\n",
    "    val_loss_avg = val_loss / len(test_loader)\n",
    "    val_losses.append(val_loss_avg)\n",
    "    val_accuracies.append(val_acc)\n",
    "    scheduler.step(val_loss_avg)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99302cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:37:00.226877Z",
     "iopub.status.busy": "2025-11-19T11:37:00.226217Z",
     "iopub.status.idle": "2025-11-19T11:37:46.407927Z",
     "shell.execute_reply": "2025-11-19T11:37:46.406990Z"
    },
    "papermill": {
     "duration": 46.194985,
     "end_time": "2025-11-19T11:37:46.414730",
     "exception": false,
     "start_time": "2025-11-19T11:37:00.219745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SwinV2 (256x256): semua hasil disimpan di /kaggle/working/\n",
      "Final Accuracy: 0.9946\n",
      "Macro F1-Score: 0.9867\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"swinv2_malimg_256.pth\"))\n",
    "\n",
    "# Evaluasi\n",
    "model.eval()\n",
    "all_preds, all_labels, inference_times = [], [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        start = time.time()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Pastikan outputs adalah 2D [batch_size, num_classes]\n",
    "        if outputs.dim() > 2:\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "        \n",
    "        end = time.time()\n",
    "        inference_times.append(end - start)\n",
    "        \n",
    "        # Prediksi\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# Metrik\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average=None, labels=range(NUM_CLASSES), zero_division=0\n",
    ")\n",
    "precision_avg = np.mean(precision)\n",
    "recall_avg = np.mean(recall)\n",
    "f1_avg = np.mean(f1)\n",
    "\n",
    "report_df = pd.DataFrame({\n",
    "    'class': class_names,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1-score': f1,\n",
    "    'support': support\n",
    "})\n",
    "report_df.to_csv(os.path.join(OUTPUT_DIR, \"SwinV2_256_per_class_metrics.csv\"), index=False)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "pd.DataFrame(cm, index=class_names, columns=class_names).to_csv(os.path.join(OUTPUT_DIR, \"SwinV2_256_confusion_matrix.csv\"))\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=False, xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - SwinV2 (256x256)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"SwinV2_256_confusion_matrix.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Acc')\n",
    "plt.plot(val_accuracies, label='Val Acc')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"SwinV2_256_training_curves.png\"))\n",
    "plt.close()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model_size_mb = total_params * 4 / (1024 ** 2)\n",
    "avg_time_per_image = np.mean(inference_times) / BATCH_SIZE\n",
    "total_inference_time = sum(inference_times)\n",
    "throughput = len(test_dataset) / total_inference_time\n",
    "\n",
    "summary = {\n",
    "    \"Model\": \"SwinV2 Base (256x256, 1-Channel, Focal Loss)\",\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Macro Precision\": precision_avg,\n",
    "    \"Macro Recall\": recall_avg,\n",
    "    \"Macro F1\": f1_avg,\n",
    "    \"Total Params\": total_params,\n",
    "    \"Trainable Params\": trainable_params,\n",
    "    \"Model Size (MB)\": model_size_mb,\n",
    "    \"Avg Inference Time (ms)\": avg_time_per_image * 1000,\n",
    "    \"Throughput (img/sec)\": throughput,\n",
    "    \"Hardware\": str(DEVICE) + (f\" ({torch.cuda.get_device_name(0)})\" if torch.cuda.is_available() else \"\")\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(os.path.join(OUTPUT_DIR, \"SwinV2_256_summary.csv\"), index=False)\n",
    "\n",
    "print(\"\\n SwinV2 (256x256): semua hasil disimpan di /kaggle/working/\")\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1-Score: {f1_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323fc7b",
   "metadata": {
    "papermill": {
     "duration": 0.005411,
     "end_time": "2025-11-19T11:37:46.425389",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.419978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf3999",
   "metadata": {
    "papermill": {
     "duration": 0.005035,
     "end_time": "2025-11-19T11:37:46.435486",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.430451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4e71b",
   "metadata": {
    "papermill": {
     "duration": 0.005111,
     "end_time": "2025-11-19T11:37:46.445604",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.440493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912e946",
   "metadata": {
    "papermill": {
     "duration": 0.005204,
     "end_time": "2025-11-19T11:37:46.456126",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.450922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc759f1",
   "metadata": {
    "papermill": {
     "duration": 0.005148,
     "end_time": "2025-11-19T11:37:46.466195",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.461047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041660e",
   "metadata": {
    "papermill": {
     "duration": 0.005227,
     "end_time": "2025-11-19T11:37:46.476327",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.471100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b0028",
   "metadata": {
    "papermill": {
     "duration": 0.005071,
     "end_time": "2025-11-19T11:37:46.486343",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.481272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75176d2",
   "metadata": {
    "papermill": {
     "duration": 0.005247,
     "end_time": "2025-11-19T11:37:46.496747",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.491500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254e026",
   "metadata": {
    "papermill": {
     "duration": 0.005229,
     "end_time": "2025-11-19T11:37:46.507068",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.501839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51eb5d3",
   "metadata": {
    "papermill": {
     "duration": 0.005249,
     "end_time": "2025-11-19T11:37:46.517600",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.512351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e587d",
   "metadata": {
    "papermill": {
     "duration": 0.005265,
     "end_time": "2025-11-19T11:37:46.528064",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.522799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb3313",
   "metadata": {
    "papermill": {
     "duration": 0.005208,
     "end_time": "2025-11-19T11:37:46.538239",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.533031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc925be",
   "metadata": {
    "papermill": {
     "duration": 0.005451,
     "end_time": "2025-11-19T11:37:46.548751",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.543300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5137dca",
   "metadata": {
    "papermill": {
     "duration": 0.005277,
     "end_time": "2025-11-19T11:37:46.559204",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.553927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b368d",
   "metadata": {
    "papermill": {
     "duration": 0.00533,
     "end_time": "2025-11-19T11:37:46.569801",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.564471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c682642",
   "metadata": {
    "papermill": {
     "duration": 0.004829,
     "end_time": "2025-11-19T11:37:46.579638",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.574809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399fa41",
   "metadata": {
    "papermill": {
     "duration": 0.005089,
     "end_time": "2025-11-19T11:37:46.589902",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.584813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb40a5",
   "metadata": {
    "papermill": {
     "duration": 0.0049,
     "end_time": "2025-11-19T11:37:46.599914",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.595014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34991719",
   "metadata": {
    "papermill": {
     "duration": 0.005139,
     "end_time": "2025-11-19T11:37:46.610051",
     "exception": false,
     "start_time": "2025-11-19T11:37:46.604912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2002344,
     "sourceId": 3310783,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11392.318438,
   "end_time": "2025-11-19T11:37:49.629672",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T08:27:57.311234",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b5e38a5b124527ba4c567709eac04f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08ff67e647d64521833173bb1617bc09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a645f74c219b42ccbdea833b3933579e",
       "placeholder": "",
       "style": "IPY_MODEL_51917ab6e3b9432ca76eace39f1b868c",
       "tabbable": null,
       "tooltip": null,
       "value": "357M/357M[00:02&lt;00:00,322MB/s]"
      }
     },
     "203d995a3bef4fd09b6f0f54f5f97c44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3318d1ac297d4f13a42823bf9ee8d180": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_03b5e38a5b124527ba4c567709eac04f",
       "max": 356962016.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d80a891dc1d747e7954ba8ccb7c0bc98",
       "tabbable": null,
       "tooltip": null,
       "value": 356962016.0
      }
     },
     "51917ab6e3b9432ca76eace39f1b868c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5f3e885848a640d8ac2fdf35f8f1f3f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "831153a9fe3346d7b1304f38f0cf3e0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9e4f11ddb2814606b8ab973ca04cd9d6",
       "placeholder": "",
       "style": "IPY_MODEL_203d995a3bef4fd09b6f0f54f5f97c44",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:100%"
      }
     },
     "9e4f11ddb2814606b8ab973ca04cd9d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a645f74c219b42ccbdea833b3933579e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b39bf715c99e4556b1b5cfe85f955bcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_831153a9fe3346d7b1304f38f0cf3e0c",
        "IPY_MODEL_3318d1ac297d4f13a42823bf9ee8d180",
        "IPY_MODEL_08ff67e647d64521833173bb1617bc09"
       ],
       "layout": "IPY_MODEL_5f3e885848a640d8ac2fdf35f8f1f3f3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d80a891dc1d747e7954ba8ccb7c0bc98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
